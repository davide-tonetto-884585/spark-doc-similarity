{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:40.714244Z",
     "start_time": "2024-05-13T17:31:40.006632Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from itertools import combinations\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, IDF, CountVectorizer, Normalizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, size, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Document Similarity\n",
    "The aim of this notebook is to calculate the similarity between documents using an approximation of the cosine similarity metric. \n",
    "In order to do this, we will use the simHash approach. \n",
    "\n",
    "Let's start by creating a Spark session."
   ],
   "id": "c426641ed66d9701"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:45.521746Z",
     "start_time": "2024-05-13T17:31:40.717741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Similar documents\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ],
   "id": "8559a366523a90c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 19:31:43 WARN Utils: Your hostname, rog-davide resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlan0)\n",
      "24/05/13 19:31:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/13 19:31:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the data\n",
    "We will use two datasets to test the similarity calculation. The first dataset is a list of mails and the second one is a list of news articles."
   ],
   "id": "69941fd6d650d1f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:51.872469Z",
     "start_time": "2024-05-13T17:31:45.523829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_mail = spark.read.parquet(\"data/mail_1.parquet\")\n",
    "\n",
    "dataset_news = spark.read.csv(\"data/news.csv\", header=True).select(\"summary\")\n",
    "dataset_news = dataset_news.withColumnRenamed(\"summary\", \"text\")"
   ],
   "id": "b29716522bb8ee7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Choose the dataset you want to use by setting the variable `dataset` to the desired dataset.",
   "id": "63b1a10ce4267013"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.490820Z",
     "start_time": "2024-05-13T17:31:51.873305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset_mail\n",
    "\n",
    "n = dataset.count()\n",
    "print(\"Number of documents: \" + str(n))"
   ],
   "id": "fcf10b5ecd17b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 231533\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data\n",
    "The first step is to preprocess the data. We will tokenize the text and remove the stop words.\n",
    "\n",
    "Remove special characters and numbers from the text."
   ],
   "id": "60829c075fb7111d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.507298Z",
     "start_time": "2024-05-13T17:31:52.492973Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = dataset.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z\\\\s]\", \"\"))",
   "id": "fd31aad7b4ef4f04",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenize the text. we will use the `Tokenizer` class from the `pyspark.ml.feature` module and store the result in a new column of the dataframe called `words`.",
   "id": "99727cdee3686e75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.650892Z",
     "start_time": "2024-05-13T17:31:52.509165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "df = tokenizer.transform(dataset)"
   ],
   "id": "2607920927bef1ae",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the empty strings from the list of words.",
   "id": "41f82947312a2574"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.678837Z",
     "start_time": "2024-05-13T17:31:52.651674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filter_udf = udf(lambda words: [word for word in words if len(word) >= 1], ArrayType(StringType()))\n",
    "df = df.withColumn('words', filter_udf(col('words')))"
   ],
   "id": "33a34ba468d9fc65",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the stop words from the list of words.",
   "id": "abcd39fd1a61fcb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.772Z",
     "start_time": "2024-05-13T17:31:52.679670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df = remover.transform(df)"
   ],
   "id": "15eef59db5223343",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the empty lists from the list of words (aka documents with no words).",
   "id": "3c1856ae861889e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:31:52.785124Z",
     "start_time": "2024-05-13T17:31:52.772911Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.filter(size(col(\"filtered\")) > 0)",
   "id": "6c0be6c93399ce4d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate the TF-IDF\n",
    "The next step is to calculate the TF-IDF of the documents. We will use the `CountVectorizer` and `IDF` classes from the `pyspark.ml.feature` module.\n",
    "\n",
    "Count the number of times each word appears in each document and store the result in a new column called `rawFeatures`. Then print the number of words in the vocabulary."
   ],
   "id": "3fa0047d08e75ae8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:32:47.421927Z",
     "start_time": "2024-05-13T17:31:52.786177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")  # , minDF=0.01)\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "\n",
    "n_words = len(cv_model.vocabulary)\n",
    "n_words"
   ],
   "id": "e8f0dd06740a4668",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the IDF of the words and store the result in a new column called `features`.",
   "id": "8942c6de09e5901d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:34.049276Z",
     "start_time": "2024-05-13T17:32:47.422810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")  # , minDocFreq=5)\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)"
   ],
   "id": "38e763892962995b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add a unique id to each document.",
   "id": "b2ae8889f08a9c76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:34.057655Z",
     "start_time": "2024-05-13T17:33:34.050129Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.withColumn('id', monotonically_increasing_id())",
   "id": "1b37251865affb86",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Show the dataframe.",
   "id": "fe9aaa9a7c27f2e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:34.562772Z",
     "start_time": "2024-05-13T17:33:34.058842Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "38b70ca4ea8473c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|                text|               words|            filtered|         rawFeatures|            features| id|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|My pleasure\\n\\n\\n...|[my, pleasure, ma...|[pleasure, mark, ...|(262144,[1,6,10,1...|(262144,[1,6,10,1...|  0|\n",
      "|Dear Dana\\n\\nThan...|[dear, dana, than...|[dear, dana, than...|(262144,[11,15,16...|(262144,[11,15,16...|  1|\n",
      "|Lew it looks like...|[lew, it, looks, ...|[lew, looks, like...|(262144,[15,20,28...|(262144,[15,20,28...|  2|\n",
      "|Have you ever see...|[have, you, ever,...|[ever, seen, site...|(262144,[240,635,...|(262144,[240,635,...|  3|\n",
      "|I am following up...|[i, am, following...|[following, someo...|(262144,[1,2,6,9,...|(262144,[1,2,6,9,...|  4|\n",
      "| Forwarded by Jud...|[forwarded, by, j...|[forwarded, judy,...|(262144,[1,3,6,9,...|(262144,[1,3,6,9,...|  5|\n",
      "|Per Kevin Moore p...|[per, kevin, moor...|[per, kevin, moor...|(262144,[3,5,10,1...|(262144,[3,5,10,1...|  6|\n",
      "|Attached is the A...|[attached, is, th...|[attached, august...|(262144,[3,13,14,...|(262144,[3,13,14,...|  7|\n",
      "|I sent an email t...|[i, sent, an, ema...|[sent, email, map...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|  8|\n",
      "|In order for UBS ...|[in, order, for, ...|[order, ubs, warb...|(262144,[3,8,14,3...|(262144,[3,8,14,3...|  9|\n",
      "|Dear Phillip \\nOu...|[dear, phillip, o...|[dear, phillip, r...|(262144,[3,5,15,1...|(262144,[3,5,15,1...| 10|\n",
      "|Thought you might...|[thought, you, mi...|[thought, might, ...|(262144,[1,2,3,5,...|(262144,[1,2,3,5,...| 11|\n",
      "|Sounds good to me...|[sounds, good, to...|[sounds, good, br...|(262144,[1,2,6,15...|(262144,[1,2,6,15...| 12|\n",
      "|I am guessing tha...|[i, am, guessing,...|[guessing, guys, ...|(262144,[14,15,24...|(262144,[14,15,24...| 13|\n",
      "|Hello Mark\\n\\nBot...|[hello, mark, bot...|[hello, mark, bob...|(262144,[1,4,5,13...|(262144,[1,4,5,13...| 14|\n",
      "|Tuesday the th is...|[tuesday, the, th...|[tuesday, th, fin...|(262144,[5,15,20,...|(262144,[5,15,20,...| 15|\n",
      "|Please decrease d...|[please, decrease...|[please, decrease...|(262144,[3,13,34,...|(262144,[3,13,34,...| 16|\n",
      "|I do not have any...|[i, do, not, have...|[negative, inform...|(262144,[1,2,6,14...|(262144,[1,2,6,14...| 17|\n",
      "|Tom\\n\\nThe confer...|[tom, the, confer...|[tom, conference,...|(262144,[1,2,6,8,...|(262144,[1,2,6,8,...| 18|\n",
      "|Original Message\\...|[original, messag...|[original, messag...|(262144,[1,2,6,10...|(262144,[1,2,6,10...| 19|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate the simHashes of the documents\n",
    "Let's calculate the simHash of the documents. \n",
    "\n",
    "Define the number of bits of the simHash and the length of the blocks."
   ],
   "id": "12a3741b54f2006d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:34.567099Z",
     "start_time": "2024-05-13T17:33:34.564960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = 64\n",
    "p = 32"
   ],
   "id": "9d332ec0ccee9013",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's define a matrix of size `m x n_words` with random values of -1 and 1 and broadcast it to all the nodes.",
   "id": "d1f27978c512bee4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:35.129170Z",
     "start_time": "2024-05-13T17:33:34.567877Z"
    }
   },
   "cell_type": "code",
   "source": "random_matrix = sc.broadcast(np.random.choice([-1, 1], (m, n_words)))",
   "id": "b6686953e75cb7b7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the function that calculates the simHash of a document.",
   "id": "22b55261a14f8270"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:33:35.135261Z",
     "start_time": "2024-05-13T17:33:35.130462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simhash(features):\n",
    "    global random_matrix, m\n",
    "\n",
    "    sim_hash = np.sum(\n",
    "        [(random_matrix.value[:, i] * features[int(i)]) for i in features.indices],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    # Check if the simHash is not a list\n",
    "    if not isinstance(sim_hash, np.ndarray):\n",
    "        return [0] * m\n",
    "\n",
    "    # Return the simHash as a list of 0s and 1s\n",
    "    return (sim_hash >= 0).astype(int).tolist()\n",
    "\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))"
   ],
   "id": "32656a9c659a5169",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the simHash of the documents and store the result in a new column called `simHash`. Then persist the dataframe.",
   "id": "86c0de2a91820968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:36:20.134728Z",
     "start_time": "2024-05-13T17:33:35.136630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_time = time()\n",
    "df = df.withColumn(\"simHash\", simhash_udf('features'))\n",
    "df.persist()\n",
    "df.collect()\n",
    "\n",
    "_time_simhash = time() - _time\n",
    "print(\"Time to calculate the simHashes: \" + str(_time_simhash))"
   ],
   "id": "7d012ccad7f5e20c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to calculate the simHashes: 164.98927474021912\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Show the dataframe.",
   "id": "343f8bff4878481d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:36:20.539751Z",
     "start_time": "2024-05-13T17:36:20.137601Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "ecb9e51a38e0fd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|                text|               words|            filtered|         rawFeatures|            features| id|             simHash|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+\n",
      "|My pleasure\\n\\n\\n...|[my, pleasure, ma...|[pleasure, mark, ...|(262144,[1,6,10,1...|(262144,[1,6,10,1...|  0|[0, 1, 0, 1, 0, 0...|\n",
      "|Dear Dana\\n\\nThan...|[dear, dana, than...|[dear, dana, than...|(262144,[11,15,16...|(262144,[11,15,16...|  1|[1, 1, 0, 1, 0, 1...|\n",
      "|Lew it looks like...|[lew, it, looks, ...|[lew, looks, like...|(262144,[15,20,28...|(262144,[15,20,28...|  2|[1, 0, 0, 1, 1, 1...|\n",
      "|Have you ever see...|[have, you, ever,...|[ever, seen, site...|(262144,[240,635,...|(262144,[240,635,...|  3|[0, 0, 1, 0, 0, 0...|\n",
      "|I am following up...|[i, am, following...|[following, someo...|(262144,[1,2,6,9,...|(262144,[1,2,6,9,...|  4|[1, 1, 0, 0, 0, 0...|\n",
      "| Forwarded by Jud...|[forwarded, by, j...|[forwarded, judy,...|(262144,[1,3,6,9,...|(262144,[1,3,6,9,...|  5|[1, 1, 1, 0, 1, 0...|\n",
      "|Per Kevin Moore p...|[per, kevin, moor...|[per, kevin, moor...|(262144,[3,5,10,1...|(262144,[3,5,10,1...|  6|[1, 0, 0, 0, 0, 0...|\n",
      "|Attached is the A...|[attached, is, th...|[attached, august...|(262144,[3,13,14,...|(262144,[3,13,14,...|  7|[0, 0, 0, 0, 1, 1...|\n",
      "|I sent an email t...|[i, sent, an, ema...|[sent, email, map...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|  8|[0, 0, 1, 1, 0, 1...|\n",
      "|In order for UBS ...|[in, order, for, ...|[order, ubs, warb...|(262144,[3,8,14,3...|(262144,[3,8,14,3...|  9|[1, 1, 0, 0, 0, 1...|\n",
      "|Dear Phillip \\nOu...|[dear, phillip, o...|[dear, phillip, r...|(262144,[3,5,15,1...|(262144,[3,5,15,1...| 10|[1, 0, 0, 1, 0, 1...|\n",
      "|Thought you might...|[thought, you, mi...|[thought, might, ...|(262144,[1,2,3,5,...|(262144,[1,2,3,5,...| 11|[1, 1, 1, 1, 1, 1...|\n",
      "|Sounds good to me...|[sounds, good, to...|[sounds, good, br...|(262144,[1,2,6,15...|(262144,[1,2,6,15...| 12|[1, 1, 1, 0, 0, 1...|\n",
      "|I am guessing tha...|[i, am, guessing,...|[guessing, guys, ...|(262144,[14,15,24...|(262144,[14,15,24...| 13|[0, 1, 1, 1, 1, 0...|\n",
      "|Hello Mark\\n\\nBot...|[hello, mark, bot...|[hello, mark, bob...|(262144,[1,4,5,13...|(262144,[1,4,5,13...| 14|[0, 0, 0, 0, 1, 1...|\n",
      "|Tuesday the th is...|[tuesday, the, th...|[tuesday, th, fin...|(262144,[5,15,20,...|(262144,[5,15,20,...| 15|[0, 0, 1, 0, 1, 0...|\n",
      "|Please decrease d...|[please, decrease...|[please, decrease...|(262144,[3,13,34,...|(262144,[3,13,34,...| 16|[0, 0, 1, 1, 1, 0...|\n",
      "|I do not have any...|[i, do, not, have...|[negative, inform...|(262144,[1,2,6,14...|(262144,[1,2,6,14...| 17|[1, 1, 1, 0, 0, 0...|\n",
      "|Tom\\n\\nThe confer...|[tom, the, confer...|[tom, conference,...|(262144,[1,2,6,8,...|(262144,[1,2,6,8,...| 18|[1, 0, 1, 0, 0, 0...|\n",
      "|Original Message\\...|[original, messag...|[original, messag...|(262144,[1,2,6,10...|(262144,[1,2,6,10...| 19|[1, 1, 0, 0, 0, 1...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate the similarity between the documents\n",
    "The next step is to calculate the similarity between the documents. \n",
    "### Split the simHashes into blocks \n",
    "Let's start by dividing the simHashes into blocks of size `p` and store them as integers. This is done in order to reduce the number of comparisons to do later."
   ],
   "id": "e4338f9384096e6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:36:20.549759Z",
     "start_time": "2024-05-13T17:36:20.541283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_blocks(simHash):\n",
    "    global m, p\n",
    "    return [int(\"\".join(map(str, simHash[i:i + p])), 2) for i in range(0, m, p)]\n",
    "\n",
    "split_blocks_udf = udf(split_blocks, ArrayType(IntegerType()))"
   ],
   "id": "d150f7036325c67b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the blocks of the simHashes and store the result in a new column called `simHashBlocks`.",
   "id": "272f7006ba34b85b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:36:20.586652Z",
     "start_time": "2024-05-13T17:36:20.551037Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.withColumn(\"simHashBlocks\", split_blocks_udf('simHash'))",
   "id": "b2d392ab186adaa8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Show the dataframe.",
   "id": "a817168a3d1efa7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:36:21.283070Z",
     "start_time": "2024-05-13T17:36:20.588656Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "b24aaa3aab242b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|                text|               words|            filtered|         rawFeatures|            features| id|             simHash|       simHashBlocks|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|My pleasure\\n\\n\\n...|[my, pleasure, ma...|[pleasure, mark, ...|(262144,[1,6,10,1...|(262144,[1,6,10,1...|  0|[0, 1, 0, 1, 0, 0...|[1359601215, -866...|\n",
      "|Dear Dana\\n\\nThan...|[dear, dana, than...|[dear, dana, than...|(262144,[11,15,16...|(262144,[11,15,16...|  1|[1, 1, 0, 1, 0, 1...|[-733771619, -352...|\n",
      "|Lew it looks like...|[lew, it, looks, ...|[lew, looks, like...|(262144,[15,20,28...|(262144,[15,20,28...|  2|[1, 0, 0, 1, 1, 1...|[-1633707351, -26...|\n",
      "|Have you ever see...|[have, you, ever,...|[ever, seen, site...|(262144,[240,635,...|(262144,[240,635,...|  3|[0, 0, 1, 0, 0, 0...|[568245094, -2063...|\n",
      "|I am following up...|[i, am, following...|[following, someo...|(262144,[1,2,6,9,...|(262144,[1,2,6,9,...|  4|[1, 1, 0, 0, 0, 0...|[-1009179959, 833...|\n",
      "| Forwarded by Jud...|[forwarded, by, j...|[forwarded, judy,...|(262144,[1,3,6,9,...|(262144,[1,3,6,9,...|  5|[1, 1, 1, 0, 1, 0...|[-384275459, 3365...|\n",
      "|Per Kevin Moore p...|[per, kevin, moor...|[per, kevin, moor...|(262144,[3,5,10,1...|(262144,[3,5,10,1...|  6|[1, 0, 0, 0, 0, 0...|[-2142395989, 128...|\n",
      "|Attached is the A...|[attached, is, th...|[attached, august...|(262144,[3,13,14,...|(262144,[3,13,14,...|  7|[0, 0, 0, 0, 1, 1...|[219933918, 13587...|\n",
      "|I sent an email t...|[i, sent, an, ema...|[sent, email, map...|(262144,[0,1,2,3,...|(262144,[0,1,2,3,...|  8|[0, 0, 1, 1, 0, 1...|[902327108, 18250...|\n",
      "|In order for UBS ...|[in, order, for, ...|[order, ubs, warb...|(262144,[3,8,14,3...|(262144,[3,8,14,3...|  9|[1, 1, 0, 0, 0, 1...|[-986291659, -162...|\n",
      "|Dear Phillip \\nOu...|[dear, phillip, o...|[dear, phillip, r...|(262144,[3,5,15,1...|(262144,[3,5,15,1...| 10|[1, 0, 0, 1, 0, 1...|[-1759707120, 170...|\n",
      "|Thought you might...|[thought, you, mi...|[thought, might, ...|(262144,[1,2,3,5,...|(262144,[1,2,3,5,...| 11|[1, 1, 1, 1, 1, 1...|[-65604100, -6052...|\n",
      "|Sounds good to me...|[sounds, good, to...|[sounds, good, br...|(262144,[1,2,6,15...|(262144,[1,2,6,15...| 12|[1, 1, 1, 0, 0, 1...|[-456735015, 5907...|\n",
      "|I am guessing tha...|[i, am, guessing,...|[guessing, guys, ...|(262144,[14,15,24...|(262144,[14,15,24...| 13|[0, 1, 1, 1, 1, 0...|[2019533338, 1929...|\n",
      "|Hello Mark\\n\\nBot...|[hello, mark, bot...|[hello, mark, bob...|(262144,[1,4,5,13...|(262144,[1,4,5,13...| 14|[0, 0, 0, 0, 1, 1...|[229890115, 19819...|\n",
      "|Tuesday the th is...|[tuesday, the, th...|[tuesday, th, fin...|(262144,[5,15,20,...|(262144,[5,15,20,...| 15|[0, 0, 1, 0, 1, 0...|[671908750, 46540...|\n",
      "|Please decrease d...|[please, decrease...|[please, decrease...|(262144,[3,13,34,...|(262144,[3,13,34,...| 16|[0, 0, 1, 1, 1, 0...|[965630371, 34477...|\n",
      "|I do not have any...|[i, do, not, have...|[negative, inform...|(262144,[1,2,6,14...|(262144,[1,2,6,14...| 17|[1, 1, 1, 0, 0, 0...|[-490671191, 1914...|\n",
      "|Tom\\n\\nThe confer...|[tom, the, confer...|[tom, conference,...|(262144,[1,2,6,8,...|(262144,[1,2,6,8,...| 18|[1, 0, 1, 0, 0, 0...|[-1553826577, 395...|\n",
      "|Original Message\\...|[original, messag...|[original, messag...|(262144,[1,2,6,10...|(262144,[1,2,6,10...| 19|[1, 1, 0, 0, 0, 1...|[-983995166, 6915...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Group the simHashes by blocks\n",
    "Now we will group togheter the blocks that shares at least one blovk in the same position and calculate the similarity between the documents in each group.\n",
    "\n",
    "Let's create a dataframe with the (id, simHashBlock, position) for each row and group by simHashBlock and position."
   ],
   "id": "78cfc1faaa6fbd93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:37:28.021043Z",
     "start_time": "2024-05-13T17:36:21.284999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_time = time()\n",
    "block_dataframe = df.select(\"id\", \"simHash\", \"simHashBlocks\", \"features\").rdd.flatMap(\n",
    "        lambda x: [((x[0], x[1], x[3]), block, i) for i, block in enumerate(x[2])]).toDF([\"id\", \"block\", \"position\"])\n",
    "\n",
    "grouped_blocks = block_dataframe.groupBy(\"block\", \"position\").agg({\"id\": \"collect_list\"})\n",
    "grouped_blocks.persist()\n",
    "grouped_blocks.collect()\n",
    "\n",
    "_time_blocks = time() - _time\n",
    "print(\"Time to group the blocks: \" + str(_time_blocks))"
   ],
   "id": "da85c6402b532926",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to group the blocks: 66.72602796554565\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add a unique id to each group.",
   "id": "de914a077c08e940"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:37:28.051243Z",
     "start_time": "2024-05-13T17:37:28.024277Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_blocks = grouped_blocks.withColumn('id', monotonically_increasing_id())",
   "id": "d66498955e073d0",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the largest group.",
   "id": "9489e76f4e3471e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:37:31.082409Z",
     "start_time": "2024-05-13T17:37:28.052927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "largest_group = grouped_blocks.orderBy(size(col(\"collect_list(id)\")).desc()).take(1)\n",
    "len(largest_group[0])"
   ],
   "id": "597a0608012112b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Show the dataframe.",
   "id": "dbf17fa3f7653e7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:37:31.267651Z",
     "start_time": "2024-05-13T17:37:31.083936Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_blocks.show()",
   "id": "7f1ca338e9873b65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+---+\n",
      "|      block|position|    collect_list(id)| id|\n",
      "+-----------+--------+--------------------+---+\n",
      "|-2145894837|       0|[{34359740938, [1...|  0|\n",
      "|-2144869491|       0|[{111669150344, [...|  1|\n",
      "|-2140903798|       0|[{74, [1, 0, 0, 0...|  2|\n",
      "|-2139806157|       0|[{85899355644, [1...|  3|\n",
      "|-2139551052|       0|[{111669154453, [...|  4|\n",
      "|-2132269696|       1|[{94489287656, [0...|  5|\n",
      "|-2131498322|       0|[{17179877741, [1...|  6|\n",
      "|-2131217808|       1|[{111669160561, [...|  7|\n",
      "|-2129690228|       1|[{8033, [1, 0, 0,...|  8|\n",
      "|-2128779444|       1|[{51539616447, [0...|  9|\n",
      "|-2128453714|       1|[{17179871495, [1...| 10|\n",
      "|-2128437929|       0|[{128849022735, [...| 11|\n",
      "|-2126092076|       0|[{17179881839, [1...| 12|\n",
      "|-2118276431|       1|[{60129548435, [1...| 13|\n",
      "|-2116252062|       1|[{85899350451, [1...| 14|\n",
      "|-2115211214|       1|[{25769810950, [0...| 15|\n",
      "|-2115209948|       1|[{85899358209, [1...| 16|\n",
      "|-2113396042|       0|[{17179878451, [1...| 17|\n",
      "|-2104882631|       1|[{103079215312, [...| 18|\n",
      "|-2099917384|       0|[{42949680728, [1...| 19|\n",
      "+-----------+--------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's compute the number of comparisons we need to do before and after grouping the blocks.",
   "id": "c6859766c7f78fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:37:41.950481Z",
     "start_time": "2024-05-13T17:37:31.268929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tot_comparison = int((n ** 2 - n) / 2)\n",
    "group_comparison = int(grouped_blocks.filter(size(col(\"collect_list(id)\")) > 1).rdd.map(lambda x: (len(x[2]) ** 2 - len(x[2])) / 2).sum())\n",
    "\n",
    "print(\"Percentage of comparisons to do: \" + str(group_comparison * 100 / tot_comparison) + \"%\")"
   ],
   "id": "75d68d8a00e74111",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=================================================>   (187 + 13) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of comparisons to do: 0.011956528630713118%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Find the similar documents\n",
    "Now we will calculate the similarity between the documents in each group and find the similar documents (aka documents with a similarity greater than 0.9).\n",
    "\n",
    "The similarity between two documents is calculated as 1 - the hamming distance between the simHashes of the documents divided by `m`."
   ],
   "id": "8e14ffca3f6f882b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:40:01.546769Z",
     "start_time": "2024-05-13T17:37:41.954171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_time = time()\n",
    "docs_to_compare = grouped_blocks.select(\"collect_list(id)\").rdd \\\n",
    "    .flatMap(lambda x: combinations(x[0], 2)).map(lambda x: (x[0][0], x[1][0], [x[0][1], x[1][1]], [x[0][2], x[1][2]])) \\\n",
    "    .toDF([\"id1\", \"id2\", \"simHashes\", \"features\"]).dropDuplicates([\"id1\", \"id2\"])\n",
    "\n",
    "similar_docs_rdd = docs_to_compare.rdd \\\n",
    "    .map(lambda x: (x[0], x[1], float(1 - np.sum([1 for i, j in zip(x[2][0], x[2][1]) if i != j]) / m))) \\\n",
    "    .filter(lambda x: x[2] >= 0.9)\n",
    "\n",
    "similar_docs = similar_docs_rdd.collect()\n",
    "\n",
    "_time_similarity = time() - _time\n",
    "print(\"Time to find similar documents:\", _time_similarity)"
   ],
   "id": "68376a525ecd2d4d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to find similar documents: 139.55639696121216\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will print the number of similar documents.",
   "id": "17cabb058033295b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:40:01.637323Z",
     "start_time": "2024-05-13T17:40:01.573022Z"
    }
   },
   "cell_type": "code",
   "source": "len(similar_docs)",
   "id": "18dd83f3d9c2aa94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740803"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print two random similar documents.",
   "id": "a82c61ad2ec85b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:40:05.506614Z",
     "start_time": "2024-05-13T17:40:01.640364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "el = similar_docs[np.random.randint(0, len(similar_docs))]\n",
    "\n",
    "print(el)\n",
    "\n",
    "print(\"Document 1:\")\n",
    "print(df.filter(col(\"id\") == el[0]).select(\"text\").collect()[0][0])\n",
    "\n",
    "print(\"\\n\\nDocument 2:\")\n",
    "print(df.filter(col(\"id\") == el[1]).select(\"text\").collect()[0][0])"
   ],
   "id": "1111aaacbb9ea106",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13008, 8589938829, 1.0)\n",
      "Document 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date  HourAhead hour   No ancillary schedules awarded  No variances detected\n",
      "\n",
      "    LOG MESSAGES\n",
      "\n",
      "PARSING FILE  OPortlandWestDeskCalifornia SchedulingISO Final Schedulestxt\n",
      "\n",
      "\n",
      "Document 2:\n",
      "Start Date  HourAhead hour   No ancillary schedules awarded  No \n",
      "variances detected\n",
      "\n",
      "    LOG MESSAGES\n",
      "\n",
      "PARSING FILE  OPortlandWestDeskCalifornia SchedulingISO Final \n",
      "Schedulestxt\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Stop the Spark session.",
   "id": "bbfe74142d258112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:40:06.518312Z",
     "start_time": "2024-05-13T17:40:05.508616Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "7ea7c8c71efd76a3",
   "outputs": [],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
